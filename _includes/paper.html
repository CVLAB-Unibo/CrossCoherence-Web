<section class="hero  has-text-centered" id="paper">
    <div class="hero-body">
        <div class="container">
          <h2 align="centered" class="title">Paper Depth4ToM</h2>
            <div class="columns">
                <div class="column is-one-fifth-desktop is-one-fifth-tablet is-one-fifth-fullhd">
                  <br>
                  <br>
                  <br>
                  <a href="https://arxiv.org/abs/2307.15052">
                  <img style="width:90%" src="assets/paper_icon.png" >
                  </a>
                </div>
                <div class="column has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen">

                    <br>
                    <div class="container columns is-centered">
                        <div>
                          <B><a href="<my_arxiv_link>"><font size = "+2">Looking at words and points with attention: a benchmark for text-to-shape coherence</font></a><br></B>
                          <B><i> Andrea Amaduzzi, Giuseppe Lisanti, Samuele Salti, Luigi Di Stefano</B></i>
                          <br>
                          <i><font size = "-1">*Equal Contribution</font></i>
                          <br>
                          <br>
                          <p>
                            While text-conditional 3D object generation and manipulation have seen rapid progress, the evaluation of coherence between generated 3D shapes and input textual descriptions
                            lacks a clear benchmark. The reason is twofold: a) the low quality of the textual descriptions in the only publicly available dataset of text-shape pairs; 
                            b) the limited effectiveness of the metrics used to quantitatively assess such coherence. In this paper, we propose a comprehensive solution that addresses both weaknesses. 
                            Firstly, we employ large language models to automatically refine textual descriptions associated with shapes. Secondly, we propose a quantitative metric to assess text-to-shape coherence,
                            through cross-attention mechanisms. To validate our approach, we conduct a user study and compare quantitatively our metric with existing ones. 
                            The refined dataset, the new metric and a set of text-shape pairs validated by the user study comprise a novel, fine-grained benchmark that we publicly release to foster research 
                            on text-to-shape coherence of text-conditioned 3D generative models.
                          </div>
                        <div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                CITATION
                </h3>
                <div class="has-text-left-desktop has-text-left-tablet has-text-left-fullhd has-text-left-widescreen form-group col-md-18 col-md-offset-0">
<pre>
@inproceedings{amaduzzi2023iccvw,
    title = {Looking at words and points with attention: a benchmark for text-to-shape coherence},
    author = {Amaduzzi, Andrea and Lisanti, Giuseppe and Salti, Samuele and Di Stefano, Luigi},
    booktitle = {2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
    note = {ICCVW},
    year = {2023},
}
</pre>
</section>
